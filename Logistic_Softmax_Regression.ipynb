{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15b64257240>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.539713 Accuracy 83.33%\n",
      "Epoch   10/1000 Cost: 0.614853 Accuracy 66.67%\n",
      "Epoch   20/1000 Cost: 0.441875 Accuracy 66.67%\n",
      "Epoch   30/1000 Cost: 0.373145 Accuracy 83.33%\n",
      "Epoch   40/1000 Cost: 0.316358 Accuracy 83.33%\n",
      "Epoch   50/1000 Cost: 0.266094 Accuracy 83.33%\n",
      "Epoch   60/1000 Cost: 0.220498 Accuracy 100.00%\n",
      "Epoch   70/1000 Cost: 0.182095 Accuracy 100.00%\n",
      "Epoch   80/1000 Cost: 0.157299 Accuracy 100.00%\n",
      "Epoch   90/1000 Cost: 0.144091 Accuracy 100.00%\n",
      "Epoch  100/1000 Cost: 0.134272 Accuracy 100.00%\n",
      "Epoch  110/1000 Cost: 0.125769 Accuracy 100.00%\n",
      "Epoch  120/1000 Cost: 0.118297 Accuracy 100.00%\n",
      "Epoch  130/1000 Cost: 0.111680 Accuracy 100.00%\n",
      "Epoch  140/1000 Cost: 0.105779 Accuracy 100.00%\n",
      "Epoch  150/1000 Cost: 0.100483 Accuracy 100.00%\n",
      "Epoch  160/1000 Cost: 0.095704 Accuracy 100.00%\n",
      "Epoch  170/1000 Cost: 0.091369 Accuracy 100.00%\n",
      "Epoch  180/1000 Cost: 0.087420 Accuracy 100.00%\n",
      "Epoch  190/1000 Cost: 0.083806 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.080486 Accuracy 100.00%\n",
      "Epoch  210/1000 Cost: 0.077425 Accuracy 100.00%\n",
      "Epoch  220/1000 Cost: 0.074595 Accuracy 100.00%\n",
      "Epoch  230/1000 Cost: 0.071969 Accuracy 100.00%\n",
      "Epoch  240/1000 Cost: 0.069526 Accuracy 100.00%\n",
      "Epoch  250/1000 Cost: 0.067248 Accuracy 100.00%\n",
      "Epoch  260/1000 Cost: 0.065118 Accuracy 100.00%\n",
      "Epoch  270/1000 Cost: 0.063122 Accuracy 100.00%\n",
      "Epoch  280/1000 Cost: 0.061247 Accuracy 100.00%\n",
      "Epoch  290/1000 Cost: 0.059483 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.057820 Accuracy 100.00%\n",
      "Epoch  310/1000 Cost: 0.056250 Accuracy 100.00%\n",
      "Epoch  320/1000 Cost: 0.054764 Accuracy 100.00%\n",
      "Epoch  330/1000 Cost: 0.053357 Accuracy 100.00%\n",
      "Epoch  340/1000 Cost: 0.052022 Accuracy 100.00%\n",
      "Epoch  350/1000 Cost: 0.050753 Accuracy 100.00%\n",
      "Epoch  360/1000 Cost: 0.049546 Accuracy 100.00%\n",
      "Epoch  370/1000 Cost: 0.048396 Accuracy 100.00%\n",
      "Epoch  380/1000 Cost: 0.047299 Accuracy 100.00%\n",
      "Epoch  390/1000 Cost: 0.046252 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045251 Accuracy 100.00%\n",
      "Epoch  410/1000 Cost: 0.044294 Accuracy 100.00%\n",
      "Epoch  420/1000 Cost: 0.043376 Accuracy 100.00%\n",
      "Epoch  430/1000 Cost: 0.042497 Accuracy 100.00%\n",
      "Epoch  440/1000 Cost: 0.041653 Accuracy 100.00%\n",
      "Epoch  450/1000 Cost: 0.040843 Accuracy 100.00%\n",
      "Epoch  460/1000 Cost: 0.040064 Accuracy 100.00%\n",
      "Epoch  470/1000 Cost: 0.039315 Accuracy 100.00%\n",
      "Epoch  480/1000 Cost: 0.038593 Accuracy 100.00%\n",
      "Epoch  490/1000 Cost: 0.037898 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037228 Accuracy 100.00%\n",
      "Epoch  510/1000 Cost: 0.036582 Accuracy 100.00%\n",
      "Epoch  520/1000 Cost: 0.035958 Accuracy 100.00%\n",
      "Epoch  530/1000 Cost: 0.035356 Accuracy 100.00%\n",
      "Epoch  540/1000 Cost: 0.034773 Accuracy 100.00%\n",
      "Epoch  550/1000 Cost: 0.034210 Accuracy 100.00%\n",
      "Epoch  560/1000 Cost: 0.033664 Accuracy 100.00%\n",
      "Epoch  570/1000 Cost: 0.033137 Accuracy 100.00%\n",
      "Epoch  580/1000 Cost: 0.032625 Accuracy 100.00%\n",
      "Epoch  590/1000 Cost: 0.032130 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031649 Accuracy 100.00%\n",
      "Epoch  610/1000 Cost: 0.031183 Accuracy 100.00%\n",
      "Epoch  620/1000 Cost: 0.030730 Accuracy 100.00%\n",
      "Epoch  630/1000 Cost: 0.030291 Accuracy 100.00%\n",
      "Epoch  640/1000 Cost: 0.029864 Accuracy 100.00%\n",
      "Epoch  650/1000 Cost: 0.029449 Accuracy 100.00%\n",
      "Epoch  660/1000 Cost: 0.029046 Accuracy 100.00%\n",
      "Epoch  670/1000 Cost: 0.028654 Accuracy 100.00%\n",
      "Epoch  680/1000 Cost: 0.028272 Accuracy 100.00%\n",
      "Epoch  690/1000 Cost: 0.027900 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027538 Accuracy 100.00%\n",
      "Epoch  710/1000 Cost: 0.027186 Accuracy 100.00%\n",
      "Epoch  720/1000 Cost: 0.026842 Accuracy 100.00%\n",
      "Epoch  730/1000 Cost: 0.026507 Accuracy 100.00%\n",
      "Epoch  740/1000 Cost: 0.026181 Accuracy 100.00%\n",
      "Epoch  750/1000 Cost: 0.025862 Accuracy 100.00%\n",
      "Epoch  760/1000 Cost: 0.025552 Accuracy 100.00%\n",
      "Epoch  770/1000 Cost: 0.025248 Accuracy 100.00%\n",
      "Epoch  780/1000 Cost: 0.024952 Accuracy 100.00%\n",
      "Epoch  790/1000 Cost: 0.024663 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024381 Accuracy 100.00%\n",
      "Epoch  810/1000 Cost: 0.024104 Accuracy 100.00%\n",
      "Epoch  820/1000 Cost: 0.023835 Accuracy 100.00%\n",
      "Epoch  830/1000 Cost: 0.023571 Accuracy 100.00%\n",
      "Epoch  840/1000 Cost: 0.023313 Accuracy 100.00%\n",
      "Epoch  850/1000 Cost: 0.023061 Accuracy 100.00%\n",
      "Epoch  860/1000 Cost: 0.022814 Accuracy 100.00%\n",
      "Epoch  870/1000 Cost: 0.022572 Accuracy 100.00%\n",
      "Epoch  880/1000 Cost: 0.022336 Accuracy 100.00%\n",
      "Epoch  890/1000 Cost: 0.022104 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.021877 Accuracy 100.00%\n",
      "Epoch  910/1000 Cost: 0.021655 Accuracy 100.00%\n",
      "Epoch  920/1000 Cost: 0.021437 Accuracy 100.00%\n",
      "Epoch  930/1000 Cost: 0.021224 Accuracy 100.00%\n",
      "Epoch  940/1000 Cost: 0.021015 Accuracy 100.00%\n",
      "Epoch  950/1000 Cost: 0.020810 Accuracy 100.00%\n",
      "Epoch  960/1000 Cost: 0.020609 Accuracy 100.00%\n",
      "Epoch  970/1000 Cost: 0.020412 Accuracy 100.00%\n",
      "Epoch  980/1000 Cost: 0.020219 Accuracy 100.00%\n",
      "Epoch  990/1000 Cost: 0.020029 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019843 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2328, 0.1150, 0.2485, 0.1661, 0.2375],\n",
      "        [0.1990, 0.1746, 0.2133, 0.1903, 0.2228],\n",
      "        [0.1709, 0.2002, 0.1985, 0.2232, 0.2072]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 원소가 0의 값을 가진 3 × 5 텐서 생성\n",
    "y_one_hot = torch.zeros_like(hypothesis) \n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2],\n",
      "        [1],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "print(y.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6347, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"softmax1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### high-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4574, -2.1628, -1.3922, -1.7949, -1.4377],\n",
       "        [-1.6144, -1.7452, -1.5452, -1.6593, -1.5013],\n",
       "        [-1.7669, -1.6085, -1.6170, -1.4995, -1.5740]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low level\n",
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4574, -2.1628, -1.3922, -1.7949, -1.4377],\n",
       "        [-1.6144, -1.7452, -1.5452, -1.6593, -1.5013],\n",
       "        [-1.7669, -1.6085, -1.6170, -1.4995, -1.5740]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6347, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low level\n",
    "# 첫번째 수식\n",
    "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6347, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두번째 수식\n",
    "(y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6347, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "# 세번째 수식\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6347, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 네번째 수식\n",
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(8, 3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # 가설\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) \n",
    "\n",
    "    # 비용 함수\n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화. 4개의 특성을 가지고 3개의 클래스로 분류. input_dim=4, output_dim=3.\n",
    "model = nn.Linear(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 4.661125\n",
      "Epoch  100/1000 Cost: 0.711834\n",
      "Epoch  200/1000 Cost: 0.630354\n",
      "Epoch  300/1000 Cost: 0.573769\n",
      "Epoch  400/1000 Cost: 0.523962\n",
      "Epoch  500/1000 Cost: 0.476843\n",
      "Epoch  600/1000 Cost: 0.430889\n",
      "Epoch  700/1000 Cost: 0.385375\n",
      "Epoch  800/1000 Cost: 0.339988\n",
      "Epoch  900/1000 Cost: 0.295069\n",
      "Epoch 1000/1000 Cost: 0.254579\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.777960\n",
      "Epoch  100/1000 Cost: 0.654127\n",
      "Epoch  200/1000 Cost: 0.561501\n",
      "Epoch  300/1000 Cost: 0.505037\n",
      "Epoch  400/1000 Cost: 0.460010\n",
      "Epoch  500/1000 Cost: 0.420253\n",
      "Epoch  600/1000 Cost: 0.383131\n",
      "Epoch  700/1000 Cost: 0.347032\n",
      "Epoch  800/1000 Cost: 0.310779\n",
      "Epoch  900/1000 Cost: 0.274060\n",
      "Epoch 1000/1000 Cost: 0.244281\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 과제 : Logistic regression과 Softmax Regression 함수 중 하나를 골라 Pure Python으로 구현하기(Sklearn, Tensorflow, Pytorch 쓰지 않고 정 안 되면 numpy까지는 사용 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "Y = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])\n",
    "w = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(w,x):\n",
    "    # x가 input으로 들어왔을 때 probability를 계산\n",
    "    eps = 0.00001\n",
    "    prob = 1/(1+np.exp(-np.dot(w,x))+eps)\n",
    "    return np.array(prob)\n",
    "\n",
    "def prediction(prob):\n",
    "    preds = np.copy(prob)\n",
    "    preds[preds>0.5]=1\n",
    "    preds[preds<0.5]=0\n",
    "    return preds\n",
    "\n",
    "def cost(x,prob,y):\n",
    "    # 예측 값과 label 사이의 cost\n",
    "    m = len(prob)\n",
    "    cost =-(1/m)*np.sum(label*-log(prob) + (1-label)*np.log(a-prob))\n",
    "    return cost\n",
    "\n",
    "def gradient(x, prob, y):\n",
    "    # gradient값 계산\n",
    "    m = len(prob)\n",
    "    grad = (1/m)*np.sum((prob-y)*x)\n",
    "    return grad\n",
    "\n",
    "def w_update(x, y, w, lr = 1e-2):\n",
    "    prob = logistic_regression(w,x)\n",
    "    pred_y = prediction(prob)\n",
    "    m = len(prob)\n",
    "    grads = gradient(x, prob, y)\n",
    "    w -= lr * grads\n",
    "    return w, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    w, pred_y = w_update(X, Y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22102565398064297"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
